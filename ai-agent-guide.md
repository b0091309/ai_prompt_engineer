# AI 智能體完整入門指南

> 本文翻譯並重新詮釋自 [DataWhale Hello-Agents 教程](https://github.com/datawhalechina/hello-agents)，以通俗易懂的方式解釋 AI 智能體的核心概念。

---

# 第一章：什麼是智能體？

## 從日常生活理解智能體

想像你有一個超級聰明的管家。這個管家能夠：
- **觀察**周圍環境發生了什麼事
- **思考**該怎麼處理眼前的狀況
- **自己決定**要採取什麼行動
- **動手去做**，然後看結果如何

這就是「智能體」（Agent）的本質——一個能夠感知環境、自主思考、採取行動來達成目標的系統。

## 智能體的四個核心元素

每個智能體都具備四個基本要素：

### 1. 環境（Environment）
智能體所處的「世界」。這個世界可以是：
- 真實的物理空間（例如掃地機器人所在的房間）
- 網路上的虛擬世界（例如能幫你訂機票的 AI 助理面對的各種網站）
- 遊戲中的虛擬場景（例如圍棋棋盤）

### 2. 感測器（Sensors）
智能體用來「感知」環境的工具，就像人的眼睛和耳朵：
- 掃地機器人的紅外線感應器
- 聊天機器人接收你輸入的文字
- 自駕車的攝影鏡頭和雷達

### 3. 執行器（Actuators）
智能體用來「做事」的工具，就像人的手和腳：
- 掃地機器人的輪子和吸塵馬達
- 聊天機器人回覆訊息的能力
- 自駕車的方向盤和油門控制

### 4. 自主性（Autonomy）
這是最關鍵的特點——智能體能夠**自己做決定**，不需要人類一步一步告訴它該怎麼做。它有自己的「判斷力」。

## 智能體的五種類型（從簡單到複雜）

### 第一種：反射型智能體
**像是什麼？** 家裡的恆溫器。

這是最簡單的智能體。它只會根據「如果...就...」的規則來行動：
- 如果溫度低於 20 度，就開暖氣
- 如果溫度高於 26 度，就開冷氣

**優點**：反應快、簡單可靠
**缺點**：很「笨」，只能處理事先設定好的情況

### 第二種：有記憶的反射型智能體
**像是什麼？** 會記得路況的導航系統。

比第一種聰明一點，它會在腦中維持一個「世界的模型」，記住一些重要的資訊：
- 記得這條路上次很塞車
- 記得這個時間點通常交通順暢

### 第三種：目標導向型智能體
**像是什麼？** 會規劃路線的 GPS 導航。

這種智能體心中有明確的「目標」，會思考「怎樣才能達成目標」：
- 目標：從台北到高雄
- 思考：走高速公路比較快，還是走省道比較省錢？
- 規劃：選擇一條最佳路線

### 第四種：效用導向型智能體
**像是什麼？** 會幫你比價的購物助理。

比目標導向更進階，它會在多個目標之間**權衡取捨**：
- 要省錢還是要省時間？
- 要品質好還是要便宜？
- 綜合考慮後，找出「整體最滿意」的選擇

### 第五種：學習型智能體
**像是什麼？** AlphaGo 圍棋程式。

最厲害的類型——它能夠從經驗中**學習進步**：
- 下了一百萬盤棋
- 記住哪些走法會贏、哪些會輸
- 越下越強，最後打敗世界冠軍

## 現代 AI 智能體的革命性突破

以上五種是傳統的智能體類型。但 2023 年以後，由**大型語言模型（LLM）** 驅動的智能體帶來了革命性的改變。

### 傳統智能體 vs LLM 智能體

| 傳統智能體 | LLM 智能體 |
|-----------|-----------|
| 需要工程師寫好每一條規則 | 透過學習海量文字自動獲得能力 |
| 只能處理預設好的情況 | 能處理從未見過的新問題 |
| 接收精確的指令 | 理解模糊的自然語言 |
| 按固定流程執行 | 能靈活調整策略 |

舉個例子：

**傳統智能體**：「請幫我查詢台北市中正區的天氣」→ 執行「查詢天氣」指令，參數為「台北市中正區」

**LLM 智能體**：「我明天想去野餐，不知道會不會下雨耶」→ 理解你的意圖，主動查詢天氣，如果會下雨還會建議你改期或準備雨具

## 智能體的三種思考方式

### 1. 反應式思考
遇到狀況**立刻反應**，不深入思考。
- 優點：速度快
- 缺點：可能做出短視的決定
- 例子：看到紅燈就停車

### 2. 規劃式思考
先**仔細思考**再行動，制定完整計畫。
- 優點：決策品質高
- 缺點：需要時間，可能反應太慢
- 例子：規劃一趟旅行的完整行程

### 3. 混合式思考（現代主流）
結合兩者：**快速反應 + 深入思考**交替進行。

這就是現代 AI 智能體採用的方式，一個循環包含：
1. **思考（Thought）**：分析現在的狀況，決定下一步
2. **行動（Action）**：執行決定的動作
3. **觀察（Observation）**：看看結果如何
4. 回到步驟 1，繼續下一輪...

## 智能體的兩種腦袋：快思考與慢思考

心理學家丹尼爾·卡尼曼提出，人類有兩套思考系統：
- **系統一**：快速、直覺、自動（例如：看到老虎就跑）
- **系統二**：緩慢、理性、費力（例如：計算 17 × 24）

現代 AI 智能體也在嘗試結合這兩種能力：
- **快系統**：神經網路的直覺判斷，處理常見情況
- **慢系統**：邏輯推理能力，處理複雜問題

## 智能體如何與環境互動

### PEAS 分析框架

設計智能體時，需要考慮四個面向：

- **P（Performance）效能指標**：怎樣算做得好？
- **E（Environment）環境**：它要在什麼環境中運作？
- **A（Actuators）執行器**：它能做什麼動作？
- **S（Sensors）感測器**：它能感知什麼？

以「旅遊規劃智能體」為例：
- 效能指標：行程是否符合預算、時間安排是否合理、景點是否有趣
- 環境：網路上的旅遊網站、地圖服務、評論平台
- 執行器：搜尋資訊、比較選項、產出行程表
- 感測器：接收使用者的需求、讀取網頁內容

### 環境的各種特性

智能體面對的環境可能有不同特性：

- **完全可觀察 vs 部分可觀察**：能看到全部資訊，還是只能看到一部分？（棋盤遊戲 vs 撲克牌）
- **確定性 vs 隨機性**：結果是否可預測？（下棋 vs 擲骰子）
- **靜態 vs 動態**：環境會不會自己改變？（填字遊戲 vs 股票市場）
- **單一智能體 vs 多智能體**：只有自己，還是有其他玩家？

## 智能體的實際運作流程

當你對一個 AI 旅遊助理說「幫我規劃三天兩夜的台南之旅」，它會這樣運作：

**第一輪循環：**
- 思考：「使用者想去台南玩三天，我需要先查詢台南有什麼景點」
- 行動：呼叫「景點搜尋工具」
- 觀察：得到台南景點清單

**第二輪循環：**
- 思考：「有了景點清單，我需要知道這幾天的天氣來安排室內外行程」
- 行動：呼叫「天氣查詢工具」
- 觀察：得到天氣預報

**第三輪循環：**
- 思考：「天氣資訊有了，現在可以安排行程。第一天下雨，安排室內景點...」
- 行動：產生完整的三日行程
- 觀察：使用者收到行程表

這個「思考-行動-觀察」的循環會不斷重複，直到任務完成。

## 智能體的兩種應用模式

### 模式一：輔助工具
智能體作為**人類的助手**，幫忙處理繁瑣的工作，但最終決定權在人類手上。

例子：
- **GitHub Copilot**：幫工程師寫程式碼
- **Claude / ChatGPT**：幫你回答問題、撰寫文章
- **Cursor**：幫你編輯和優化程式

### 模式二：自主代理
智能體**獨立完成任務**，甚至多個智能體之間互相協作。

例子：
- 自動化客服系統：全程處理客戶問題，必要時才轉人工
- 自動化研究助理：自己搜尋資料、閱讀論文、整理報告
- 多智能體協作：一個負責寫程式、一個負責測試、一個負責審核

## 「工作流」與「智能體」的差別

很多人會混淆這兩個概念：

### 工作流（Workflow）
像是**食譜**——預先定義好每一個步驟，按順序執行：
1. 收到訂單
2. 檢查庫存
3. 處理付款
4. 安排出貨

不管發生什麼狀況，都按照這個流程走。如果遇到意外（例如庫存不足），就需要人類介入。

### 智能體（Agent）
像是**有經驗的員工**——知道最終目標是什麼，會根據實際情況靈活應對：
- 如果庫存不足，自動聯繫供應商
- 如果付款失敗，嘗試其他付款方式
- 如果遇到特殊狀況，自己判斷該怎麼處理

## 智能體的四大核心能力

1. **任務分解**：把複雜的大任務拆成小步驟
2. **工具使用**：知道什麼時候該用什麼工具
3. **上下文理解**：記得對話的來龍去脈
4. **結果整合**：把各種資訊整理成有用的答案

---

# 第二章：智能體的發展歷史

## 為什麼要了解歷史？

了解 AI 智能體的發展歷史，能幫助我們理解：
- 現在的技術是怎麼來的
- 為什麼會發展成現在這個樣子
- 過去踩過哪些坑，現在如何避免

## 第一個時代：符號主義（1950-1980年代）

### 核心思想：智能就是符號運算

最早的 AI 科學家認為：**只要把知識用符號表示出來，再用邏輯規則來推理，就能創造出智能。**

這就像是教電腦玩一個超大的「如果...那麼...」遊戲：
- 如果病人發燒 + 咳嗽 + 喉嚨痛 → 那麼可能是感冒
- 如果病人發燒 + 咳嗽 + 呼吸困難 → 那麼可能是肺炎

### 物理符號系統假說

1976 年，兩位 AI 先驅艾倫·紐厄爾（Allen Newell）和赫伯特·西蒙（Herbert Simon）提出了一個大膽的理論：

> 任何具有通用智能的系統，本質上都是一個「物理符號系統」——能夠創造、操作、組合符號的系統。

換句話說，他們認為人類的思考，本質上就是在腦中操作符號。如果電腦也能操作符號，就能擁有智能。

### 專家系統：符號主義的巔峰

基於這個思想，1970-80 年代誕生了「專家系統」——把專家的知識整理成規則庫，讓電腦模仿專家做決策。

**MYCIN 系統**是最著名的例子，它是一個醫療診斷系統：
- 儲存了約 600 條醫學規則
- 能診斷血液感染疾病
- 準確率達到人類專家的水準

工程師們興奮地認為，這就是通往通用人工智慧的道路！

### SHRDLU：早期的「理解」嘗試

1970 年，特里·威諾格拉德（Terry Winograd）開發了一個叫 SHRDLU 的系統。這個系統能在一個虛擬的「積木世界」中理解人類的指令：

人類說：「把紅色的積木放到藍色的積木上面」
SHRDLU：理解指令 → 找到紅色積木 → 找到藍色積木 → 執行移動

這看起來很厲害，SHRDLU 好像真的「理解」了人類的語言！

但問題是...

### 符號主義的致命缺陷

這些系統遇到了幾個根本性的問題：

**問題一：知識獲取瓶頸**

專家系統需要人類專家把所有知識「說出來」，然後工程師再把這些知識轉換成規則。但是：
- 很多知識是「隱性」的，專家自己也說不清楚
- 規則寫不完，現實世界太複雜了
- 維護這些規則是一場噩夢

想像一下，你要教電腦「什麼是貓」：
- 貓有四條腿 → 但是狗也有四條腿
- 貓會喵喵叫 → 但是有些貓不叫
- 貓有尾巴 → 但是有些貓沒有尾巴
- ...

規則越寫越多，例外情況也越來越多，最後變成一團亂。

**問題二：框架問題**

當智能體執行一個動作後，世界上有什麼改變了？什麼沒有改變？

例如：「我把杯子從桌上拿起來」
- 改變了：杯子的位置、我手上有東西了
- 沒改變：房間的溫度、窗外的天氣、其他所有物品的位置...

要列出「什麼沒改變」幾乎是不可能的任務！但符號系統需要知道這些才能正確推理。

**問題三：脆弱性**

只要遇到規則庫沒有涵蓋的情況，系統就完全傻掉了。

SHRDLU 在「積木世界」中表現優異，但它只懂積木。問它「今天天氣如何？」——它完全無法理解。

現實世界不是一個簡單的積木世界。

## 第一個聊天機器人：ELIZA

### 1966 年的突破

約瑟夫·魏澤鮑姆（Joseph Weizenbaum）在 1966 年創造了 ELIZA——歷史上第一個聊天機器人。

ELIZA 模仿心理治療師的對話方式：

> 使用者：我最近很難過
> ELIZA：你說你很難過，能多告訴我一些嗎？
>
> 使用者：因為我媽媽不理解我
> ELIZA：說說看你媽媽的事？
>
> 使用者：她總是批評我
> ELIZA：你覺得她為什麼總是批評你呢？

### ELIZA 的運作原理

ELIZA 其實很「笨」，它只是在做**模式匹配**：

1. **找關鍵字**：掃描使用者輸入，找特定詞彙（如「媽媽」「難過」「我覺得」）
2. **套用模板**：根據關鍵字選擇回應模板
3. **文字替換**：把「我」換成「你」，把「我的」換成「你的」
4. **輸出回應**

例如，看到「我覺得 X」，就回應「為什麼你覺得 X 呢？」

### ELIZA 的啟示

ELIZA 根本不「理解」任何東西，但很多使用者卻相信 ELIZA 真的在傾聽他們！有些人甚至對 ELIZA 產生了情感依附。

這個現象被稱為「ELIZA 效應」——人類傾向於把智能和情感投射到能對話的機器上。

這也提醒我們：**「能對話」不等於「能理解」**。

## 馬文·明斯基的革命性思想

### 心智社會理論

AI 先驅馬文·明斯基（Marvin Minsky）在 1986 年提出了一個顛覆性的觀點：

> 智能不是一個單一的整體，而是由許許多多「不怎麼聰明」的小單元協作而成。

就像一個螞蟻群落——單隻螞蟻很笨，但整個螞蟻群卻展現出驚人的集體智慧。

### 用搭積木塔來理解

假設你要「搭一座積木塔」：
- 你的大腦中有一個「搭塔」模組被啟動
- 「搭塔」模組啟動「建造者」模組
- 「建造者」啟動「添加積木」模組
- 「添加積木」啟動「找積木」和「放積木」模組
- 「放積木」啟動「移動手臂」「鬆開手指」等底層模組

每個模組都很簡單，但組合起來就能完成複雜的任務。

### 對現代 AI 的影響

明斯基的「心智社會」理論啟發了：
- **多智能體系統**：讓多個專門的 AI 協作，而不是打造一個萬能 AI
- **分散式人工智慧**：智能分布在多個節點，而非集中在一處
- **湧現智能**：簡單規則的互動，產生複雜的智能行為

這與現代的「多智能體框架」（如 CrewAI、MetaGPT）不謀而合！

## 第二個時代：連結主義與機器學習（1980年代-2010年代）

### 從「寫規則」到「讓機器學習」

符號主義的失敗讓研究者意識到：與其費力寫規則，不如讓機器自己從數據中學習！

這就是「連結主義」的核心思想——模仿人腦的神經網路結構：
- 人腦有數十億個神經元相互連結
- 學習就是調整神經元之間的連結強度
- 電腦也可以模仿這個機制

### 神經網路的運作

想像一個簡單的神經網路學習「辨認貓的照片」：

1. 輸入：一張照片（轉換成數字）
2. 經過多層「人工神經元」處理
3. 輸出：「這是貓」或「這不是貓」

一開始，網路的判斷很隨機。但透過大量的「訓練」：
- 看到貓的照片，標記「是貓」
- 看到狗的照片，標記「不是貓」
- 每次判斷錯誤，就調整神經元之間的連結強度

經過數百萬張照片的訓練，網路就「學會」了辨認貓！

### 神經網路的優勢

- **不需要手動寫規則**：規則自動從數據中學習出來
- **能處理模糊情況**：不是非黑即白，而是給出機率
- **泛化能力強**：能辨認從未見過的新照片

### 強化學習：從試錯中學習

另一種重要的學習方式是「強化學習」——透過**獎勵與懲罰**來學習。

想像訓練一隻狗：
- 做對了，給狗零食（獎勵）
- 做錯了，不給零食（懲罰）
- 狗會逐漸學會哪些行為能獲得獎勵

AI 智能體也能用這種方式學習！

**AlphaGo** 就是最著名的例子：
- 讓 AI 自己跟自己下棋，下了數百萬盤
- 贏了就獎勵，輸了就懲罰
- 最終學會了超越人類的棋藝

### 連結主義的挑戰

雖然神經網路很強大，但也有缺點：
- **黑箱問題**：無法解釋「為什麼」做出這個決定
- **需要大量數據**：沒有足夠的訓練數據就學不好
- **可能學到錯誤的東西**：如果訓練數據有偏見，模型也會有偏見

## 第三個時代：大型語言模型（2017年至今）

### 預訓練：讓 AI 閱讀整個網路

2017 年之後，AI 進入了一個新時代。

研究者發現，如果讓神經網路閱讀**海量的文字**——網頁、書籍、論文、對話——它會自動學會：
- 語言的規律（文法、用詞）
- 世界的知識（歷史、科學、常識）
- 推理的能力（邏輯思考、問題解決）

這就是「預訓練」的威力。GPT、Claude、Gemini 這些大型語言模型，都是用這種方式訓練出來的。

### 湧現能力：量變產生質變

有趣的是，當模型規模達到一定程度後，會突然展現出**訓練時沒有刻意教過**的能力：
- 能夠進行複雜的邏輯推理
- 能夠理解並遵循複雜的指令
- 能夠從幾個例子就學會新任務
- 能夠進行「思維鏈」推理（一步一步思考）

這就像是「量變引起質變」——參數夠多、數據夠大，新的能力就自然「湧現」出來。

### LLM 驅動的智能體

把大型語言模型作為智能體的「大腦」，就誕生了現代的 AI 智能體：

- **感知模組**：接收使用者的問題或環境的資訊
- **大腦（LLM）**：理解問題、進行推理、制定計畫
- **工具模組**：呼叫搜尋引擎、計算器、資料庫等外部工具
- **執行模組**：輸出結果或執行動作

這種架構讓智能體能夠：
- 理解自然語言的模糊指令
- 靈活應對各種情況
- 使用各種工具來完成任務
- 從互動中學習和調整

### 發展歷程的啟示

回顧 AI 智能體的發展歷史，我們可以看到：

1. **每個新技術都是為了解決上一代的問題**
   - 符號主義無法處理模糊情況 → 神經網路登場
   - 神經網路需要大量標註數據 → 預訓練技術出現
   - 單一模型能力有限 → 多智能體協作興起

2. **三大思潮的融合**
   - 符號主義：提供邏輯推理能力
   - 連結主義：提供學習能力
   - 大語言模型：提供知識和語言能力

現代最先進的 AI 智能體，正是這三者的結合。

---

# 第三章：大型語言模型基礎

## 什麼是語言模型？

簡單來說，語言模型就是一個能**預測下一個字**的 AI。

給它一句話的開頭：「今天天氣真...」
它會預測接下來最可能的字是「好」、「差」、「熱」...

看起來很簡單，但這個簡單的任務，需要 AI 具備：
- 對語言規律的理解
- 對世界知識的掌握
- 對上下文的記憶

當這個能力變得非常強大時，就誕生了 ChatGPT、Claude 這樣的 AI。

## 語言模型的演進之路

### 第一代：N-gram 模型（統計方法）

最早的語言模型很簡單：統計「哪些字經常一起出現」。

例如，統計大量文本後發現：
- 「天氣」後面常常是「很好」「不錯」「晴朗」
- 「我愛」後面常常是「你」「吃」「台灣」

這就是「N-gram」模型——看前 N 個字，預測下一個字。

**問題**：只能看很短的上下文，無法理解長句子的意思。

### 第二代：循環神經網路（RNN/LSTM）

這種模型有「記憶」能力，能記住前面讀過的內容。

想像你在讀一本書，RNN 就像是邊讀邊做筆記，記住前面的情節，才能理解後面的內容。

**問題**：記憶力有限，讀到後面就忘了前面；而且只能一個字一個字處理，速度慢。

### 第三代：Transformer（革命性突破）

2017 年，Google 提出了 Transformer 架構，徹底改變了遊戲規則。

Transformer 的核心創新是「自注意力機制」——**同時看整段文字的所有部分，判斷哪些部分最相關**。

舉例：「小明昨天買了一隻貓，他非常喜歡**牠**。」

當模型要理解「牠」指的是什麼時：
- 傳統模型：要從頭讀到「牠」，靠記憶推斷
- Transformer：直接「看」整句話，發現「牠」和「貓」最相關

這讓 Transformer 能夠：
- **平行處理**：一次處理整段文字，速度快很多
- **長距離關聯**：句首和句尾的詞也能輕鬆關聯
- **更深的理解**：捕捉複雜的語義關係

現在的 GPT、Claude、Gemini 全都基於 Transformer 架構。

## Transformer 的核心概念（用白話解釋）

### 自注意力機制：誰和誰最相關？

想像你在開會，會議室裡有 10 個人在說話。

「自注意力」就是讓你判斷：**對於你現在要理解的這句話，誰說的話最重要？**

- 如果在討論技術問題，工程師的話最重要
- 如果在討論預算，財務的話最重要
- 如果在討論時程，專案經理的話最重要

Transformer 對每個字都做這樣的判斷，決定要「注意」哪些其他的字。

### 多頭注意力：從不同角度看

一個「注意力頭」可能只注意到一種關係（例如：文法上的關係）。

多頭注意力就是**同時從多個角度來分析**：
- 一個頭注意文法關係
- 一個頭注意語義關係
- 一個頭注意代名詞指代
- 一個頭注意情感色彩
- ...

最後把所有角度的理解整合起來。

### 位置編碼：記住誰先誰後

因為 Transformer 是「同時」看所有的字，它原本不知道字的順序。

位置編碼就是給每個字一個「位置標籤」，告訴模型：這個字是第 1 個、那個字是第 5 個...

### 逐字生成：一個字一個字蹦出來

當你問 ChatGPT 問題時，它是**一個字一個字生成**回答的：

1. 預測第 1 個字，選機率最高的
2. 把第 1 個字加入輸入，預測第 2 個字
3. 把前 2 個字加入輸入，預測第 3 個字
4. ...重複直到生成完整回答

這就是為什麼你能看到 AI 的回答「逐漸出現」。

## 如何與大語言模型互動

### 分詞：把文字切成小塊

模型不是一個字一個字讀的，而是把文字切成「詞元（Token）」。

例如：「人工智慧很厲害」可能被切成：
「人工」「智慧」「很」「厲害」

或者用更細的切法：
「人」「工」「智」「慧」「很」「厲」「害」

現代模型使用「子詞切分」，在這兩種極端之間取得平衡：
- 常見的詞保持完整（「人工智慧」）
- 罕見的詞拆成更小的單位（「厲害」→「厲」「害」）

**為什麼這很重要？**
- 詞元數量影響處理速度和成本
- 切分方式影響模型的理解能力
- 許多 API 是按詞元數量收費的

### 提示詞工程：如何問對問題

「提示詞（Prompt）」就是你給 AI 的指令或問題。

同樣的任務，不同的提示詞，效果差很多：

**不好的提示詞**：
「寫一篇文章」

**好的提示詞**：
「請幫我寫一篇 500 字的科普文章，主題是『為什麼天空是藍色的』，目標讀者是國中生，語氣要活潑有趣，請用生活中的例子來解釋。」

### 幾種常見的提示技巧

**零樣本提示（Zero-shot）**：直接問問題，不給例子
「請把這段英文翻譯成中文：Hello, how are you?」

**少樣本提示（Few-shot）**：先給幾個例子，再問問題
「英文：Hello → 中文：你好
英文：Thank you → 中文：謝謝
英文：Good morning → 中文：？」

**思維鏈提示（Chain-of-Thought）**：請 AI 一步一步思考
「請一步一步推理，然後給出答案：小明有 5 顆蘋果，給了小華 2 顆，又買了 3 顆，現在有幾顆？」

**角色扮演**：給 AI 一個身份
「你是一位經驗豐富的營養師，請回答以下問題...」

### 控制生成的隨機性

你可以調整 AI 回答的「創意程度」：

**Temperature（溫度）**：
- 低溫度（0.1-0.3）：回答較固定、保守、事實性強
- 高溫度（0.7-1.0）：回答較多樣、創意、可能天馬行空

**適用場景**：
- 寫程式、回答事實問題 → 低溫度
- 寫故事、腦力激盪 → 高溫度

## 如何選擇適合的模型

現在市面上有很多大語言模型可選，該怎麼挑？

### 主要考量因素

1. **能力與任務匹配**
   - 需要強大推理能力 → 選擇頂級模型（GPT-4、Claude Opus）
   - 簡單對話任務 → 較小模型就夠用

2. **成本與預算**
   - 頂級模型較貴
   - 可以根據任務難度選擇不同等級

3. **速度要求**
   - 需要即時回應 → 選擇較快的模型
   - 可以等待 → 選擇較強的模型

4. **隱私考量**
   - 敏感數據 → 考慮本地部署的開源模型
   - 一般用途 → 雲端 API 方便又強大

5. **上下文長度**
   - 需要處理長文件 → 選擇支援長上下文的模型
   - 短對話 → 一般長度就足夠

### 主流模型概覽

**閉源模型（使用 API）**：
- OpenAI：GPT-4、GPT-4o
- Anthropic：Claude 3 系列
- Google：Gemini 系列

**開源模型（可本地部署）**：
- Meta：Llama 系列
- 阿里：Qwen 系列
- 其他：Mistral、ChatGLM 等

## 大語言模型的局限性

雖然很強大，但 LLM 並非完美，了解其局限性很重要。

### 幻覺問題（最重要的限制！）

「幻覺」是指 AI **一本正經地說出錯誤或虛構的資訊**。

例如：
- 問它一本不存在的書，它可能會編造內容
- 問它某人的經歷，它可能會捏造細節
- 問它專業問題，它可能會給出錯誤但看起來很專業的答案

**為什麼會發生？**

因為語言模型的本質是「預測最可能的下一個字」，而不是「查證事實」。它會生成「看起來合理」的文字，即使那些內容是錯的。

**如何應對？**

1. **永遠要驗證重要資訊**：特別是事實性內容
2. **使用 RAG（檢索增強生成）**：讓 AI 先搜尋資料再回答
3. **給予可靠的參考資料**：讓 AI 基於你提供的資料回答
4. **使用外部工具**：讓 AI 用搜尋引擎、計算器等驗證

### 知識的時效性

LLM 只知道**訓練資料截止日期之前**的事情。

如果模型的訓練數據截止於 2024 年 1 月，那麼：
- 2024 年之後發生的事，它不知道
- 2024 年之後的新技術、新聞、價格，它不知道

**解決方案**：讓智能體能夠上網搜尋最新資訊。

### 偏見問題

訓練數據來自人類產生的文字，如果這些文字帶有偏見，模型也會學到這些偏見：
- 性別偏見
- 種族偏見
- 文化偏見

使用 AI 時需要意識到這個問題，特別是在做重要決策時。

## 規模法則：越大越強？

研究者發現，語言模型的能力遵循一定的規律：

- **模型越大**（參數越多）→ 能力越強
- **數據越多** → 能力越強
- **訓練時間越長** → 能力越強

這就是為什麼大公司不斷推出更大的模型。

但也有研究發現，**數據品質比數量更重要**。用高品質的小數據集，可能比低品質的大數據集效果更好。

## 為什麼理解這些對使用 AI 智能體很重要？

1. **選擇合適的模型**：了解不同模型的優缺點，選擇最適合你任務的
2. **寫出好的提示詞**：理解模型如何「思考」，才能更好地引導它
3. **預期合理的結果**：知道模型的能力邊界，不會期待它做不可能的事
4. **設計驗證機制**：知道幻覺問題的存在，就會設計檢查機制
5. **理解成本和速度**：分詞、上下文長度直接影響 API 費用和回應速度

---

# 總結：踏入 AI 智能體的世界

## 三個章節的核心重點

### 第一章告訴我們
智能體是能夠**感知、思考、行動**的自主系統。現代 AI 智能體由大型語言模型驅動，具備理解自然語言、靈活推理、使用工具的能力。

### 第二章告訴我們
AI 經歷了從**符號主義**到**機器學習**再到**大語言模型**的演進。每一代技術都是為了解決上一代的局限。理解歷史能幫助我們理解現在，預見未來。

### 第三章告訴我們
大語言模型的核心是**預測下一個字**，但這個簡單的任務需要深刻的語言理解和知識。Transformer 架構是革命性的突破，但模型也有**幻覺**等局限，使用時需要謹慎。

## 給初學者的建議

1. **不要被術語嚇到**：AI 的核心概念其實不難理解
2. **動手嘗試**：用 ChatGPT、Claude 等工具實際體驗
3. **保持批判思考**：AI 很強大，但不是萬能的
4. **持續學習**：這個領域發展很快，保持好奇心

## 接下來的旅程

有了這三章的基礎，你已經準備好深入學習：
- 如何設計和建造自己的 AI 智能體
- 如何讓多個智能體協同工作
- 如何在實際專案中應用這些技術

歡迎來到 AI 智能體的世界！

---

*本文內容翻譯並重新詮釋自 [DataWhale Hello-Agents 開源教程](https://github.com/datawhalechina/hello-agents)，以通俗易懂的繁體中文呈現。*
